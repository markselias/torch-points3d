import numpy as np
import torch
import random

from torch_points3d.datasets.base_dataset import BaseDataset, save_used_properties
from torch_points3d.datasets.segmentation.sbub import SBUBData
import torch_points3d.core.data_transform as cT
from torch_points3d.metrics.panoptic_tracker import PanopticTracker
from torch_points3d.datasets.panoptic.utils import set_extra_labels
from torch_geometric.nn import knn_interpolate
from torch_points3d.core.data_transform import SaveOriginalPosId

CLASSES_INV = {
    1: "plants"
}


class PanopticSBUBBase:
    INSTANCE_CLASSES = CLASSES_INV.keys()
    NUM_MAX_OBJECTS = 64

    def __getitem__(self, idx):
        """
        Data object contains:
            pos - points
            x - features
        """
        if not isinstance(idx, int):
            raise ValueError("Only integer indices supported")

        # Get raw data and apply transforms
        data = super().__getitem__(idx)
        
        features = torch.ones_like(data["instance_labels"])
        data.x = features.float().unsqueeze(dim=-1)

        # Extract instance and box labels
        self._set_extra_labels(data)
        return data

    def _set_extra_labels(self, data):
        return set_extra_labels(data, self.INSTANCE_CLASSES, self.NUM_MAX_OBJECTS)

    @property
    def stuff_classes(self):
        return torch.tensor([])


class PanopticSBUBData(PanopticSBUBBase, SBUBData):
    def process(self):
        super().process()

    def download(self):
        super().download()


class SBUBDataset(BaseDataset):
    """ Wrapper around S3DISSphere that creates train and test datasets.

    http://buildingparser.stanford.edu/dataset.html

    Parameters
    ----------
    dataset_opt: omegaconf.DictConfig
        Config dictionary that should contain

            - dataroot
            - fold: test_area parameter
            - pre_collate_transform
            - train_transforms
            - test_transforms
    """

    def __init__(self, dataset_opt):
        super().__init__(dataset_opt)

        dataset_cls = PanopticSBUBData

        self.train_dataset = dataset_cls(
            self._data_path,
            sample_per_epoch=1000,
            split="train",
            pre_collate_transform=self.pre_collate_transform,
            transform=self.train_transform,
        )

        self.val_dataset = dataset_cls(
            self._data_path,
            sample_per_epoch=1000,
            split="val",
            pre_collate_transform=self.pre_collate_transform,
            transform=self.val_transform,
        )
        self.test_dataset = dataset_cls(
            self._data_path,
            sample_per_epoch=1000,
            split="test",
            pre_collate_transform=self.pre_collate_transform,
            transform=self.test_transform,
        )

    @property
    def test_data(self):
        return self.test_dataset[0].raw_test_data

    @property  # type: ignore
    @save_used_properties
    def stuff_classes(self):
        """ Returns a list of classes that are not instances
        """
        return self.train_dataset.stuff_classes

    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):
        """Factory method for the tracker

        Arguments:
            wandb_log - Log using weight and biases
            tensorboard_log - Log using tensorboard
        Returns:
            [BaseTracker] -- tracker
        """

        return PanopticTracker(self, wandb_log=wandb_log, use_tensorboard=tensorboard_log)
    
    def predict_original_samples(self, batch, conv_type, output):
        """ Takes the output generated by the NN and upsamples it to the original data
        Arguments:
            batch -- processed batch
            conv_type -- Type of convolutio (DENSE, PARTIAL_DENSE, etc...)
            output -- output predicted by the model
        """
        full_res_results = {}
        num_sample = BaseDataset.get_num_samples(batch, conv_type)
        if conv_type == "DENSE":
            output = output.reshape(num_sample, -1, output.shape[-1])  # [B,N,L]

        setattr(batch, "_pred", output)
        for sample_id in range(num_sample):
            sampleid = batch.sampleid[sample_id]
            sample_raw_pos = self.test_dataset[0]._get_item(sampleid).pos.to(batch.pos.device)
            predicted = BaseDataset.get_sample(batch, "_pred", torch.tensor(sample_id), conv_type)
            origindid = BaseDataset.get_sample(batch, SaveOriginalPosId.KEY, sample_id, conv_type)
            full_prediction = knn_interpolate(predicted, sample_raw_pos[origindid], sample_raw_pos, k=3)
            labels = full_prediction.max(1)[1].unsqueeze(-1)
            full_res_results[self.test_dataset[0].get_filename(sampleid)] = np.hstack(
                (sample_raw_pos.cpu().numpy(), labels.cpu().numpy(),)
            )
        return full_res_results

